{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71ec6487-45d1-429b-980d-27ea0ccea4be",
   "metadata": {},
   "source": [
    "# End to End Transformers\n",
    "\n",
    "The transformers API can handle all the tokenization, conversion to input ids, padding, truncation and the attention masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02be084a-91eb-47c1-91fd-124a8f98124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs=tokenizer(sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371d8eff-3b32-41d2-b895-7e4862b2b800",
   "metadata": {},
   "source": [
    "## Multiple Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59b109e0-e13f-46e3-8213-2e84fbf8d832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This can also handle multiple sequences with no code change\n",
    "\n",
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]\n",
    "\n",
    "model_inputs = tokenizer(sequences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c5c3f5-0412-4c80-8dcc-494040c724d4",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec6048d5-23c6-4252-8d76-fe08b587013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the maximum sequence length\n",
    "model_inputs = tokenizer(sequences, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59f51a4e-7bac-47fb-9aaa-f4f4f279ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the model max length\n",
    "# (512 for BERT or DistilBERT)\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "142a98bd-59b0-4485-8ca7-eb4d3c552e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will pad the sequences up to the specified max length\n",
    "model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ef7219-889e-47fb-8108-4632a8dce64f",
   "metadata": {},
   "source": [
    "## Truncating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "404870b9-4a83-4eb6-95dd-e9e207429ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "887f1817-2a61-44f2-8269-151d3697a1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will truncate sequences that are longer than the maximum model lenghth (512 for BERT)\n",
    "model_inputs = tokenizer(sequences, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "699254a6-a47f-4bfd-abcd-1e26beefdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will truncate sequences that are longer than the maximum length specified\n",
    "model_inputs = tokenizer(sequences,max_length=8,truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcc2003-1718-45f9-a812-32c73615cdce",
   "metadata": {},
   "source": [
    "## Framework specific tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c8d2b01-68f0-4f7c-9088-ca0322685a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return PyTorch Tensors\n",
    "model_inputs = tokenizer(sequences,padding=True, return_tensors='pt')\n",
    "\n",
    "# Return Tensorflow Tensors\n",
    "#model_inputs = tokenizer(sequences,padding=True, return_tensors='tf')\n",
    "# Returns NumPy arrays\n",
    "#model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "78229462-052d-47dd-a531-2447e9d13731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172,\n",
      "          2607,  2026,  2878,  2166,  1012,   102],\n",
      "        [  101,  2061,  2031,  1045,   999,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0]])\n"
     ]
    }
   ],
   "source": [
    "print(model_inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a8dae3-3694-4eee-a0ea-0e05c01e7ce7",
   "metadata": {},
   "source": [
    "## Special Tokens\n",
    "There are some special tokens that are added by the tokenizer api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0d1fdcd-9928-4371-b1ea-544365242457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102]\n",
      "[1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012]\n"
     ]
    }
   ],
   "source": [
    "sequence = \"I've been waiting for a HuggingFace course my whole life.\"\n",
    "\n",
    "model_inputs = tokenizer(sequence)\n",
    "print(model_inputs[\"input_ids\"])\n",
    "\n",
    "tokens = tokenizer.tokenize(sequence)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a86dfe9-1fda-464c-999d-6b6c0700cb40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] i've been waiting for a huggingface course my whole life. [SEP]\n"
     ]
    }
   ],
   "source": [
    "# Decode to check them \n",
    "print(tokenizer.decode(model_inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f239c9b9-f839-4b51-aa77-41724f30d35f",
   "metadata": {},
   "source": [
    "These special tokens are because the model was pretrained with these"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface:Python",
   "language": "python",
   "name": "conda-env-huggingface-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
