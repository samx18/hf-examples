{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f8e7814-ab65-495d-9c29-ccde08c343ea",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a7f5af9-565f-4e72-a1e2-89ec9a0d27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Package imports\n",
    "import torch\n",
    "from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f14a78e5-b788-4aca-a405-053bda75a217",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "checkpoint = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "\n",
    "sequences = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"This course is amazing!\",\n",
    "]\n",
    "\n",
    "# get model inputs or batches\n",
    "batch = tokenizer(sequences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "batch[\"labels\"] = torch.tensor([1,1])\n",
    "\n",
    "optimizer = AdamW(model.parameters())\n",
    "\n",
    "loss = model(**batch).loss\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a765d0-b529-4381-838f-d2ef2f9dbf55",
   "metadata": {},
   "source": [
    "## Downloading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5d37f54-5176-4a60-898c-a22859695a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Solving environment: failed with repodata from current_repodata.json, will retry with next repodata source.\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "  current version: 4.10.3\n",
      "  latest version: 4.11.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base conda\n",
      "\n",
      "\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /home/studio-lab-user/.conda/envs/huggingface\n",
      "\n",
      "  added / updated specs:\n",
      "    - datasets\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    abseil-cpp-20210324.2      |       h9c3ff4c_0        1010 KB  conda-forge\n",
      "    aiohttp-3.8.1              |   py39h3811e60_0         588 KB  conda-forge\n",
      "    aiosignal-1.2.0            |     pyhd8ed1ab_0          12 KB  conda-forge\n",
      "    arrow-cpp-6.0.1            |py39h2c67c1e_5_cpu        24.6 MB  conda-forge\n",
      "    async-timeout-4.0.2        |     pyhd8ed1ab_0           9 KB  conda-forge\n",
      "    attrs-21.4.0               |     pyhd8ed1ab_0          49 KB  conda-forge\n",
      "    aws-c-auth-0.6.8           |       hadad3cd_1          94 KB  conda-forge\n",
      "    aws-c-cal-0.5.12           |       h70efedd_7          36 KB  conda-forge\n",
      "    aws-c-common-0.6.17        |       h7f98852_0         179 KB  conda-forge\n",
      "    aws-c-compression-0.2.14   |       h7c7754b_7          17 KB  conda-forge\n",
      "    aws-c-event-stream-0.2.7   |      hd2be095_32          47 KB  conda-forge\n",
      "    aws-c-http-0.6.10          |       h416565a_3         173 KB  conda-forge\n",
      "    aws-c-io-0.10.14           |       he836878_0         137 KB  conda-forge\n",
      "    aws-c-mqtt-0.7.10          |       h885097b_0          67 KB  conda-forge\n",
      "    aws-c-s3-0.1.29            |       h8d70ed6_0          49 KB  conda-forge\n",
      "    aws-c-sdkutils-0.1.1       |       h7c7754b_4          23 KB  conda-forge\n",
      "    aws-checksums-0.1.12       |       h7c7754b_6          50 KB  conda-forge\n",
      "    aws-crt-cpp-0.17.10        |       h6ab17b9_5         207 KB  conda-forge\n",
      "    aws-sdk-cpp-1.9.160        |       h36ff4c5_0         4.6 MB  conda-forge\n",
      "    brotlipy-0.7.0             |py39h3811e60_1003         342 KB  conda-forge\n",
      "    c-ares-1.18.1              |       h7f98852_0         113 KB  conda-forge\n",
      "    cffi-1.15.0                |   py39h4bc2ebd_0         227 KB  conda-forge\n",
      "    charset-normalizer-2.0.11  |     pyhd8ed1ab_0          35 KB  conda-forge\n",
      "    colorama-0.4.4             |     pyh9f0ad1d_0          18 KB  conda-forge\n",
      "    cryptography-36.0.1        |   py39h95dcef6_0         1.7 MB  conda-forge\n",
      "    datasets-1.18.3            |     pyhd8ed1ab_0         230 KB  conda-forge\n",
      "    dill-0.3.4                 |     pyhd8ed1ab_0          62 KB  conda-forge\n",
      "    filelock-3.4.2             |     pyhd8ed1ab_1          12 KB  conda-forge\n",
      "    frozenlist-1.3.0           |   py39h3811e60_0         161 KB  conda-forge\n",
      "    fsspec-2022.1.0            |     pyhd8ed1ab_0          91 KB  conda-forge\n",
      "    gflags-2.2.2               |    he1b5a44_1004         114 KB  conda-forge\n",
      "    glog-0.5.0                 |       h48cff8f_0         104 KB  conda-forge\n",
      "    grpc-cpp-1.42.0            |       ha1441d3_1         4.0 MB  conda-forge\n",
      "    huggingface_hub-0.4.0      |     pyhd8ed1ab_0          53 KB  conda-forge\n",
      "    idna-3.3                   |     pyhd8ed1ab_0          55 KB  conda-forge\n",
      "    importlib-metadata-4.10.1  |   py39hf3d152e_0          33 KB  conda-forge\n",
      "    importlib_metadata-4.10.1  |       hd8ed1ab_0           4 KB  conda-forge\n",
      "    libcurl-7.81.0             |       h2574ce0_0         338 KB  conda-forge\n",
      "    libev-4.33                 |       h516909a_1         104 KB  conda-forge\n",
      "    libnghttp2-1.46.0          |       h812cca2_0         806 KB  conda-forge\n",
      "    libprotobuf-3.19.4         |       h780b84a_0         2.6 MB  conda-forge\n",
      "    libssh2-1.10.0             |       ha56f1ee_2         233 KB  conda-forge\n",
      "    libthrift-0.15.0           |       he6d91bd_1         4.6 MB  conda-forge\n",
      "    libutf8proc-2.7.0          |       h7f98852_0          98 KB  conda-forge\n",
      "    multidict-6.0.2            |   py39h3811e60_0         156 KB  conda-forge\n",
      "    multiprocess-0.70.12.2     |   py39h3811e60_1         210 KB  conda-forge\n",
      "    orc-1.7.1                  |       h1be678f_1         1.1 MB  conda-forge\n",
      "    pandas-1.4.0               |   py39hde0f152_0        44.1 MB  conda-forge\n",
      "    parquet-cpp-1.5.1          |                2           3 KB  conda-forge\n",
      "    pyarrow-6.0.1              |py39hff6fa39_5_cpu         2.9 MB  conda-forge\n",
      "    pycparser-2.21             |     pyhd8ed1ab_0         100 KB  conda-forge\n",
      "    pyopenssl-22.0.0           |     pyhd8ed1ab_0          49 KB  conda-forge\n",
      "    pysocks-1.7.1              |   py39hf3d152e_4          28 KB  conda-forge\n",
      "    python-xxhash-2.0.2        |   py39h3811e60_1          21 KB  conda-forge\n",
      "    pytz-2021.3                |     pyhd8ed1ab_0         242 KB  conda-forge\n",
      "    pyyaml-6.0                 |   py39h3811e60_3         192 KB  conda-forge\n",
      "    re2-2021.11.01             |       h9c3ff4c_0         222 KB  conda-forge\n",
      "    requests-2.27.1            |     pyhd8ed1ab_0          53 KB  conda-forge\n",
      "    s2n-1.3.0                  |       h9b69904_0         483 KB  conda-forge\n",
      "    snappy-1.1.8               |       he1b5a44_3          32 KB  conda-forge\n",
      "    tqdm-4.62.3                |     pyhd8ed1ab_0          80 KB  conda-forge\n",
      "    typing-extensions-4.0.1    |       hd8ed1ab_0           8 KB  conda-forge\n",
      "    urllib3-1.26.8             |     pyhd8ed1ab_1         100 KB  conda-forge\n",
      "    xxhash-0.8.0               |       h7f98852_3          86 KB  conda-forge\n",
      "    yaml-0.2.5                 |       h7f98852_2          87 KB  conda-forge\n",
      "    yarl-1.7.2                 |   py39h3811e60_1         138 KB  conda-forge\n",
      "    zipp-3.7.0                 |     pyhd8ed1ab_1          12 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:        98.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  abseil-cpp         conda-forge/linux-64::abseil-cpp-20210324.2-h9c3ff4c_0\n",
      "  aiohttp            conda-forge/linux-64::aiohttp-3.8.1-py39h3811e60_0\n",
      "  aiosignal          conda-forge/noarch::aiosignal-1.2.0-pyhd8ed1ab_0\n",
      "  arrow-cpp          conda-forge/linux-64::arrow-cpp-6.0.1-py39h2c67c1e_5_cpu\n",
      "  async-timeout      conda-forge/noarch::async-timeout-4.0.2-pyhd8ed1ab_0\n",
      "  attrs              conda-forge/noarch::attrs-21.4.0-pyhd8ed1ab_0\n",
      "  aws-c-auth         conda-forge/linux-64::aws-c-auth-0.6.8-hadad3cd_1\n",
      "  aws-c-cal          conda-forge/linux-64::aws-c-cal-0.5.12-h70efedd_7\n",
      "  aws-c-common       conda-forge/linux-64::aws-c-common-0.6.17-h7f98852_0\n",
      "  aws-c-compression  conda-forge/linux-64::aws-c-compression-0.2.14-h7c7754b_7\n",
      "  aws-c-event-stream conda-forge/linux-64::aws-c-event-stream-0.2.7-hd2be095_32\n",
      "  aws-c-http         conda-forge/linux-64::aws-c-http-0.6.10-h416565a_3\n",
      "  aws-c-io           conda-forge/linux-64::aws-c-io-0.10.14-he836878_0\n",
      "  aws-c-mqtt         conda-forge/linux-64::aws-c-mqtt-0.7.10-h885097b_0\n",
      "  aws-c-s3           conda-forge/linux-64::aws-c-s3-0.1.29-h8d70ed6_0\n",
      "  aws-c-sdkutils     conda-forge/linux-64::aws-c-sdkutils-0.1.1-h7c7754b_4\n",
      "  aws-checksums      conda-forge/linux-64::aws-checksums-0.1.12-h7c7754b_6\n",
      "  aws-crt-cpp        conda-forge/linux-64::aws-crt-cpp-0.17.10-h6ab17b9_5\n",
      "  aws-sdk-cpp        conda-forge/linux-64::aws-sdk-cpp-1.9.160-h36ff4c5_0\n",
      "  brotlipy           conda-forge/linux-64::brotlipy-0.7.0-py39h3811e60_1003\n",
      "  c-ares             conda-forge/linux-64::c-ares-1.18.1-h7f98852_0\n",
      "  cffi               conda-forge/linux-64::cffi-1.15.0-py39h4bc2ebd_0\n",
      "  charset-normalizer conda-forge/noarch::charset-normalizer-2.0.11-pyhd8ed1ab_0\n",
      "  colorama           conda-forge/noarch::colorama-0.4.4-pyh9f0ad1d_0\n",
      "  cryptography       conda-forge/linux-64::cryptography-36.0.1-py39h95dcef6_0\n",
      "  datasets           conda-forge/noarch::datasets-1.18.3-pyhd8ed1ab_0\n",
      "  dill               conda-forge/noarch::dill-0.3.4-pyhd8ed1ab_0\n",
      "  filelock           conda-forge/noarch::filelock-3.4.2-pyhd8ed1ab_1\n",
      "  frozenlist         conda-forge/linux-64::frozenlist-1.3.0-py39h3811e60_0\n",
      "  fsspec             conda-forge/noarch::fsspec-2022.1.0-pyhd8ed1ab_0\n",
      "  gflags             conda-forge/linux-64::gflags-2.2.2-he1b5a44_1004\n",
      "  glog               conda-forge/linux-64::glog-0.5.0-h48cff8f_0\n",
      "  grpc-cpp           conda-forge/linux-64::grpc-cpp-1.42.0-ha1441d3_1\n",
      "  huggingface_hub    conda-forge/noarch::huggingface_hub-0.4.0-pyhd8ed1ab_0\n",
      "  idna               conda-forge/noarch::idna-3.3-pyhd8ed1ab_0\n",
      "  importlib-metadata conda-forge/linux-64::importlib-metadata-4.10.1-py39hf3d152e_0\n",
      "  importlib_metadata conda-forge/noarch::importlib_metadata-4.10.1-hd8ed1ab_0\n",
      "  libcurl            conda-forge/linux-64::libcurl-7.81.0-h2574ce0_0\n",
      "  libev              conda-forge/linux-64::libev-4.33-h516909a_1\n",
      "  libnghttp2         conda-forge/linux-64::libnghttp2-1.46.0-h812cca2_0\n",
      "  libprotobuf        conda-forge/linux-64::libprotobuf-3.19.4-h780b84a_0\n",
      "  libssh2            conda-forge/linux-64::libssh2-1.10.0-ha56f1ee_2\n",
      "  libthrift          conda-forge/linux-64::libthrift-0.15.0-he6d91bd_1\n",
      "  libutf8proc        conda-forge/linux-64::libutf8proc-2.7.0-h7f98852_0\n",
      "  multidict          conda-forge/linux-64::multidict-6.0.2-py39h3811e60_0\n",
      "  multiprocess       conda-forge/linux-64::multiprocess-0.70.12.2-py39h3811e60_1\n",
      "  orc                conda-forge/linux-64::orc-1.7.1-h1be678f_1\n",
      "  pandas             conda-forge/linux-64::pandas-1.4.0-py39hde0f152_0\n",
      "  parquet-cpp        conda-forge/noarch::parquet-cpp-1.5.1-2\n",
      "  pyarrow            conda-forge/linux-64::pyarrow-6.0.1-py39hff6fa39_5_cpu\n",
      "  pycparser          conda-forge/noarch::pycparser-2.21-pyhd8ed1ab_0\n",
      "  pyopenssl          conda-forge/noarch::pyopenssl-22.0.0-pyhd8ed1ab_0\n",
      "  pysocks            conda-forge/linux-64::pysocks-1.7.1-py39hf3d152e_4\n",
      "  python-xxhash      conda-forge/linux-64::python-xxhash-2.0.2-py39h3811e60_1\n",
      "  pytz               conda-forge/noarch::pytz-2021.3-pyhd8ed1ab_0\n",
      "  pyyaml             conda-forge/linux-64::pyyaml-6.0-py39h3811e60_3\n",
      "  re2                conda-forge/linux-64::re2-2021.11.01-h9c3ff4c_0\n",
      "  requests           conda-forge/noarch::requests-2.27.1-pyhd8ed1ab_0\n",
      "  s2n                conda-forge/linux-64::s2n-1.3.0-h9b69904_0\n",
      "  snappy             conda-forge/linux-64::snappy-1.1.8-he1b5a44_3\n",
      "  tqdm               conda-forge/noarch::tqdm-4.62.3-pyhd8ed1ab_0\n",
      "  typing-extensions  conda-forge/noarch::typing-extensions-4.0.1-hd8ed1ab_0\n",
      "  urllib3            conda-forge/noarch::urllib3-1.26.8-pyhd8ed1ab_1\n",
      "  xxhash             conda-forge/linux-64::xxhash-0.8.0-h7f98852_3\n",
      "  yaml               conda-forge/linux-64::yaml-0.2.5-h7f98852_2\n",
      "  yarl               conda-forge/linux-64::yarl-1.7.2-py39h3811e60_1\n",
      "  zipp               conda-forge/noarch::zipp-3.7.0-pyhd8ed1ab_1\n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages\n",
      "aws-c-io-0.10.14     | 137 KB    | ##################################### | 100% \n",
      "aws-checksums-0.1.12 | 50 KB     | ##################################### | 100% \n",
      "gflags-2.2.2         | 114 KB    | ##################################### | 100% \n",
      "brotlipy-0.7.0       | 342 KB    | ##################################### | 100% \n",
      "aws-sdk-cpp-1.9.160  | 4.6 MB    | ##################################### | 100% \n",
      "pysocks-1.7.1        | 28 KB     | ##################################### | 100% \n",
      "aws-c-auth-0.6.8     | 94 KB     | ##################################### | 100% \n",
      "pyarrow-6.0.1        | 2.9 MB    | ##################################### | 100% \n",
      "libthrift-0.15.0     | 4.6 MB    | ##################################### | 100% \n",
      "aiosignal-1.2.0      | 12 KB     | ##################################### | 100% \n",
      "libutf8proc-2.7.0    | 98 KB     | ##################################### | 100% \n",
      "importlib_metadata-4 | 4 KB      | ##################################### | 100% \n",
      "requests-2.27.1      | 53 KB     | ##################################### | 100% \n",
      "yaml-0.2.5           | 87 KB     | ##################################### | 100% \n",
      "arrow-cpp-6.0.1      | 24.6 MB   | ##################################### | 100% \n",
      "grpc-cpp-1.42.0      | 4.0 MB    | ##################################### | 100% \n",
      "typing-extensions-4. | 8 KB      | ##################################### | 100% \n",
      "filelock-3.4.2       | 12 KB     | ##################################### | 100% \n",
      "multidict-6.0.2      | 156 KB    | ##################################### | 100% \n",
      "datasets-1.18.3      | 230 KB    | ##################################### | 100% \n",
      "tqdm-4.62.3          | 80 KB     | ##################################### | 100% \n",
      "async-timeout-4.0.2  | 9 KB      | ##################################### | 100% \n",
      "xxhash-0.8.0         | 86 KB     | ##################################### | 100% \n",
      "aws-c-cal-0.5.12     | 36 KB     | ##################################### | 100% \n",
      "aws-c-common-0.6.17  | 179 KB    | ##################################### | 100% \n",
      "attrs-21.4.0         | 49 KB     | ##################################### | 100% \n",
      "charset-normalizer-2 | 35 KB     | ##################################### | 100% \n",
      "pytz-2021.3          | 242 KB    | ##################################### | 100% \n",
      "c-ares-1.18.1        | 113 KB    | ##################################### | 100% \n",
      "yarl-1.7.2           | 138 KB    | ##################################### | 100% \n",
      "pycparser-2.21       | 100 KB    | ##################################### | 100% \n",
      "aws-c-sdkutils-0.1.1 | 23 KB     | ##################################### | 100% \n",
      "cffi-1.15.0          | 227 KB    | ##################################### | 100% \n",
      "python-xxhash-2.0.2  | 21 KB     | ##################################### | 100% \n",
      "colorama-0.4.4       | 18 KB     | ##################################### | 100% \n",
      "fsspec-2022.1.0      | 91 KB     | ##################################### | 100% \n",
      "libcurl-7.81.0       | 338 KB    | ##################################### | 100% \n",
      "huggingface_hub-0.4. | 53 KB     | ##################################### | 100% \n",
      "libev-4.33           | 104 KB    | ##################################### | 100% \n",
      "libssh2-1.10.0       | 233 KB    | ##################################### | 100% \n",
      "frozenlist-1.3.0     | 161 KB    | ##################################### | 100% \n",
      "libprotobuf-3.19.4   | 2.6 MB    | ##################################### | 100% \n",
      "re2-2021.11.01       | 222 KB    | ##################################### | 100% \n",
      "libnghttp2-1.46.0    | 806 KB    | ##################################### | 100% \n",
      "zipp-3.7.0           | 12 KB     | ##################################### | 100% \n",
      "s2n-1.3.0            | 483 KB    | ##################################### | 100% \n",
      "aws-c-http-0.6.10    | 173 KB    | ##################################### | 100% \n",
      "cryptography-36.0.1  | 1.7 MB    | ##################################### | 100% \n",
      "aiohttp-3.8.1        | 588 KB    | ##################################### | 100% \n",
      "dill-0.3.4           | 62 KB     | ##################################### | 100% \n",
      "pyopenssl-22.0.0     | 49 KB     | ##################################### | 100% \n",
      "aws-crt-cpp-0.17.10  | 207 KB    | ##################################### | 100% \n",
      "importlib-metadata-4 | 33 KB     | ##################################### | 100% \n",
      "parquet-cpp-1.5.1    | 3 KB      | ##################################### | 100% \n",
      "pyyaml-6.0           | 192 KB    | ##################################### | 100% \n",
      "orc-1.7.1            | 1.1 MB    | ##################################### | 100% \n",
      "abseil-cpp-20210324. | 1010 KB   | ##################################### | 100% \n",
      "snappy-1.1.8         | 32 KB     | ##################################### | 100% \n",
      "aws-c-mqtt-0.7.10    | 67 KB     | ##################################### | 100% \n",
      "multiprocess-0.70.12 | 210 KB    | ##################################### | 100% \n",
      "pandas-1.4.0         | 44.1 MB   | ##################################### | 100% \n",
      "aws-c-compression-0. | 17 KB     | ##################################### | 100% \n",
      "idna-3.3             | 55 KB     | ##################################### | 100% \n",
      "urllib3-1.26.8       | 100 KB    | ##################################### | 100% \n",
      "aws-c-s3-0.1.29      | 49 KB     | ##################################### | 100% \n",
      "glog-0.5.0           | 104 KB    | ##################################### | 100% \n",
      "aws-c-event-stream-0 | 47 KB     | ##################################### | 100% \n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install the HF datasets library\n",
    "%conda install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d01584fe-5425-4554-b9ef-e3a4bf414138",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13641c66-17e9-483e-be8e-b591ea89bf50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee5342e9de7f4a039ed62d2934ed6a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/7.78k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a8251358c304c6cb1cb0945b9390ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset glue/mrpc (download: 1.43 MiB, generated: 1.43 MiB, post-processed: Unknown size, total: 2.85 MiB) to /home/studio-lab-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c91fe733bbc45399ba1d4c0b6b94f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a188e7980dbc462b83d413d941bcacc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca7ef2ee57041bd9627eeca706f4131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd9bf9934bdb49ecbeb0cc207c23125d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset glue downloaded and prepared to /home/studio-lab-user/.cache/huggingface/datasets/glue/mrpc/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bd261ad7e7c4ba0a5e76604438fc74d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the mrpc dataset\n",
    "raw_datasets = load_dataset(\"glue\", \"mrpc\")\n",
    "raw_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40949df3-9bd6-4d88-9cc9-785b33726b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': 'Amrozi accused his brother , whom he called \" the witness \" , of deliberately distorting his evidence .',\n",
       " 'sentence2': 'Referring to him as only \" the witness \" , Amrozi accused his brother of deliberately distorting his evidence .',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first element of the train data\n",
    "raw_train_dataset = raw_datasets[\"train\"]\n",
    "raw_train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8d9419f-c164-4201-8a00-f73ab8f9f2d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence1': Value(dtype='string', id=None),\n",
       " 'sentence2': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=2, names=['not_equivalent', 'equivalent'], names_file=None, id=None),\n",
       " 'idx': Value(dtype='int32', id=None)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check features of the train data set\n",
    "raw_train_dataset.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8419d75d-8842-4738-8938-2c8773a59332",
   "metadata": {},
   "source": [
    "## Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82f1fdd6-d99e-4506-a5af-9176669c0eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "tokenized_sentences_1 = tokenizer(raw_datasets[\"train\"][\"sentence1\"])\n",
    "tokenized_sentences_2 = tokenizer(raw_datasets[\"train\"][\"sentence2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c15d19-daa8-4ea2-92ce-fca255407920",
   "metadata": {},
   "source": [
    "### Sequences\n",
    "Since we cannot pass two sentences to a model and get a prediction if they are related, we need to pass them as sequence pairs. The BERT tokenizer can accept sequences of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39cd6150-fd6a-4401-a057-af14c8643c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2023, 2003, 1996, 2034, 6251, 102, 2023, 2003, 1996, 2117, 6251, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(\"This is the first sentence\",\"This is the second sentence\")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c806d22-d445-41a5-ab1d-5d84362af422",
   "metadata": {},
   "source": [
    "The token_type_ids indicate which sequence the token belongs "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad987da-19fb-4e57-830c-026bf365ea2f",
   "metadata": {},
   "source": [
    "_Take element 15 of the training set and tokenize the two sentences separately and as a pair. What’s the difference between the two results?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd8c6971-d567-4a54-8563-885e41a4a6bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Senior Vice President Eric Rudder , formerly head of the Developer and Platform Evangelism unit , will lead the new entity .'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_dataset[15][\"sentence2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bda4d5a0-8ba0-4c4a-9b4e-63cfa88a07e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 24049, 2001, 2087, 3728, 3026, 3580, 2343, 2005, 1996, 9722, 1004, 4132, 9340, 12439, 2964, 2449, 1012, 102, 3026, 3580, 2343, 4388, 24049, 1010, 3839, 2132, 1997, 1996, 9722, 1998, 4132, 9340, 12439, 2964, 3131, 1010, 2097, 2599, 1996, 2047, 9178, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs=tokenizer(raw_train_dataset[15][\"sentence1\"],raw_train_dataset[15][\"sentence2\"])\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "08ecc2ea-2ebb-4933-9470-e450e9881def",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'rudder',\n",
       " 'was',\n",
       " 'most',\n",
       " 'recently',\n",
       " 'senior',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'for',\n",
       " 'the',\n",
       " 'developer',\n",
       " '&',\n",
       " 'platform',\n",
       " 'evan',\n",
       " '##gel',\n",
       " '##ism',\n",
       " 'business',\n",
       " '.',\n",
       " '[SEP]',\n",
       " 'senior',\n",
       " 'vice',\n",
       " 'president',\n",
       " 'eric',\n",
       " 'rudder',\n",
       " ',',\n",
       " 'formerly',\n",
       " 'head',\n",
       " 'of',\n",
       " 'the',\n",
       " 'developer',\n",
       " 'and',\n",
       " 'platform',\n",
       " 'evan',\n",
       " '##gel',\n",
       " '##ism',\n",
       " 'unit',\n",
       " ',',\n",
       " 'will',\n",
       " 'lead',\n",
       " 'the',\n",
       " 'new',\n",
       " 'entity',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert back to tokens\n",
    "tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2af4bb-f801-4dcb-9747-a1cd9ed1b8a2",
   "metadata": {},
   "source": [
    "### Dataset map function\n",
    "To keep the data as dataset, we can use the dataset.map() method which works by applying a function to each element of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64194e37-b669-4ff5-a07c-5b182de62d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(example):\n",
    "    return tokenizer(example[\"sentence1\"], example[\"sentence2\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "645b49eb-ea42-4786-af9e-f41aa187ac64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "128f94c4d9044c8dbe725c0d4b90a644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a41d09c4fc74994bff0725c88b7346d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d427c4c2ec724b81b4fa195eb99c9d07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 3668\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 408\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label', 'idx', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 1725\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)\n",
    "tokenized_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1bd85ee-0382-40cb-ae54-fc75751d9c7a",
   "metadata": {},
   "source": [
    "## Padding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e24518-9f37-4db1-ac09-4b5b2fa68d80",
   "metadata": {},
   "source": [
    "### Dynamic Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ea925bab-d29b-47be-9803-e83026ba934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19135658-c86f-4a2c-a988-3dabcd721f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ecadbc18-bfd1-447e-bb69-7b0fa58aa3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, 59, 47, 67, 59, 50, 62, 32]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the first 8 samples from the dataset\n",
    "samples = tokenized_datasets[\"train\"][:8]\n",
    "\n",
    "samples = {k: v for k, v in samples.items() if k not in [\"idx\", \"sentence1\", \"sentence2\"]}\n",
    "[len(x) for x in samples[\"input_ids\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d63fcc4c-6fdb-4f39-a41b-62a0d6495058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': torch.Size([8, 67]),\n",
       " 'token_type_ids': torch.Size([8, 67]),\n",
       " 'attention_mask': torch.Size([8, 67]),\n",
       " 'labels': torch.Size([8])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The longest sentence in the batch is 67 so padding is applied to meet that length\n",
    "batch = data_collator(samples)\n",
    "{k: v.shape for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fafb826-d6a8-4bf1-818b-de06a52562af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "huggingface:Python",
   "language": "python",
   "name": "conda-env-huggingface-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
